#!/bin/bash
# gpu-mode - Switch between Gaming Mode and ML Mode on k3s-gpu-worker
#
# Story: 13.2 - Configure Mode Switching Script
# FR97: Operator can switch between Gaming Mode and ML Mode via script
# FR98: Gaming Mode scales vLLM pods to 0 and enables CPU fallback
# FR99: ML Mode restores vLLM pods when gaming exits
# NFR51: Gaming Mode activation <30 seconds
# NFR52: ML Mode restoration <2 minutes
#
# Usage:
#   gpu-mode gaming   - Scale vLLM to 0, release GPU for Steam
#   gpu-mode ml       - Scale vLLM to 1, restore ML inference
#   gpu-mode status   - Show current mode and GPU status

set -e

# Configuration
NAMESPACE="ml"
DEPLOYMENT="vllm-server"
OLLAMA_DEPLOYMENT="ollama"
TIMEOUT_GAMING=30   # NFR51: <30 seconds
TIMEOUT_ML=120      # NFR52: <2 minutes

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[OK]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Get current vLLM replica count
get_vllm_replicas() {
    kubectl get deployment "$DEPLOYMENT" -n "$NAMESPACE" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0"
}

# Get current vLLM ready replicas
get_vllm_ready() {
    kubectl get deployment "$DEPLOYMENT" -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0"
}

# Check if Ollama is available
check_ollama() {
    local ready
    ready=$(kubectl get deployment "$OLLAMA_DEPLOYMENT" -n "$NAMESPACE" -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
    if [[ "$ready" -ge 1 ]]; then
        echo "available"
    else
        echo "unavailable"
    fi
}

# Get GPU memory usage
get_gpu_memory() {
    if command -v nvidia-smi &> /dev/null; then
        nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits 2>/dev/null | head -1
    else
        echo "N/A"
    fi
}

# Gaming Mode: Scale vLLM to 0
gaming_mode() {
    local start_time
    start_time=$(date +%s)

    log_info "Activating Gaming Mode..."

    # Check current state
    local current_replicas
    current_replicas=$(get_vllm_replicas)

    if [[ "$current_replicas" == "0" ]]; then
        log_warn "vLLM already scaled to 0 - Gaming Mode already active"
        echo ""
        echo -e "${GREEN}Gaming Mode: vLLM scaled to 0, GPU available for Steam${NC}"
        return 0
    fi

    # Scale down vLLM
    log_info "Scaling vLLM to 0 replicas..."
    kubectl scale deployment "$DEPLOYMENT" -n "$NAMESPACE" --replicas=0

    # Wait for pods to terminate
    log_info "Waiting for vLLM pods to terminate..."
    local elapsed=0
    while [[ $elapsed -lt $TIMEOUT_GAMING ]]; do
        local pods
        pods=$(kubectl get pods -n "$NAMESPACE" -l app=vllm-server --no-headers 2>/dev/null | wc -l)
        if [[ "$pods" -eq 0 ]]; then
            break
        fi
        sleep 2
        elapsed=$((elapsed + 2))
    done

    # Verify termination
    local remaining_pods
    remaining_pods=$(kubectl get pods -n "$NAMESPACE" -l app=vllm-server --no-headers 2>/dev/null | wc -l)

    local end_time
    end_time=$(date +%s)
    local duration=$((end_time - start_time))

    if [[ "$remaining_pods" -eq 0 ]]; then
        log_success "vLLM pods terminated"

        # Check GPU memory
        local gpu_mem
        gpu_mem=$(get_gpu_memory)
        log_info "GPU memory: ${gpu_mem} MiB (used/total)"

        # Check Ollama fallback
        local ollama_status
        ollama_status=$(check_ollama)
        if [[ "$ollama_status" == "available" ]]; then
            log_success "Ollama CPU fallback: available"
        else
            log_warn "Ollama CPU fallback: unavailable"
        fi

        echo ""
        echo -e "${GREEN}Gaming Mode: vLLM scaled to 0, GPU available for Steam${NC}"
        echo -e "Completion time: ${duration}s (target: <${TIMEOUT_GAMING}s)"

        if [[ $duration -lt $TIMEOUT_GAMING ]]; then
            log_success "NFR51 met: Completion time ${duration}s < ${TIMEOUT_GAMING}s"
        else
            log_warn "NFR51 exceeded: Completion time ${duration}s >= ${TIMEOUT_GAMING}s"
        fi
    else
        log_error "vLLM pods still running after ${TIMEOUT_GAMING}s timeout"
        return 1
    fi
}

# ML Mode: Scale vLLM to 1
ml_mode() {
    local start_time
    start_time=$(date +%s)

    log_info "Activating ML Mode..."

    # Check current state
    local current_replicas
    current_replicas=$(get_vllm_replicas)
    local ready_replicas
    ready_replicas=$(get_vllm_ready)

    if [[ "$current_replicas" -ge 1 ]] && [[ "$ready_replicas" -ge 1 ]]; then
        log_warn "vLLM already running with ${ready_replicas} ready replica(s) - ML Mode already active"
        echo ""
        echo -e "${GREEN}ML Mode: vLLM scaled to 1, GPU available for inference${NC}"
        return 0
    fi

    # Scale up vLLM
    log_info "Scaling vLLM to 1 replica..."
    kubectl scale deployment "$DEPLOYMENT" -n "$NAMESPACE" --replicas=1

    # Wait for pod to be ready
    log_info "Waiting for vLLM pod to be ready (model loading may take ~60-90s)..."
    local elapsed=0
    while [[ $elapsed -lt $TIMEOUT_ML ]]; do
        ready_replicas=$(get_vllm_ready)
        if [[ "$ready_replicas" -ge 1 ]]; then
            break
        fi

        # Show progress
        local pod_status
        pod_status=$(kubectl get pods -n "$NAMESPACE" -l app=vllm-server --no-headers 2>/dev/null | awk '{print $3}' | head -1)
        echo -ne "\r  Status: ${pod_status:-Pending} (${elapsed}s elapsed)    "

        sleep 5
        elapsed=$((elapsed + 5))
    done
    echo "" # New line after progress

    local end_time
    end_time=$(date +%s)
    local duration=$((end_time - start_time))

    ready_replicas=$(get_vllm_ready)
    if [[ "$ready_replicas" -ge 1 ]]; then
        log_success "vLLM pod ready"

        # Check GPU memory
        local gpu_mem
        gpu_mem=$(get_gpu_memory)
        log_info "GPU memory: ${gpu_mem} MiB (used/total)"

        # Test health endpoint
        log_info "Verifying vLLM health..."
        if kubectl exec -n "$NAMESPACE" deploy/ollama -- curl -s -o /dev/null -w "%{http_code}" "http://vllm-api.${NAMESPACE}.svc.cluster.local:8000/health" --connect-timeout 5 2>/dev/null | grep -q "200"; then
            log_success "vLLM health check: passed"
        else
            log_warn "vLLM health check: could not verify (may still be loading)"
        fi

        echo ""
        echo -e "${GREEN}ML Mode: vLLM scaled to 1, GPU available for inference${NC}"
        echo -e "Completion time: ${duration}s (target: <${TIMEOUT_ML}s)"

        if [[ $duration -lt $TIMEOUT_ML ]]; then
            log_success "NFR52 met: Completion time ${duration}s < ${TIMEOUT_ML}s"
        else
            log_warn "NFR52 exceeded: Completion time ${duration}s >= ${TIMEOUT_ML}s"
        fi
    else
        log_error "vLLM pod not ready after ${TIMEOUT_ML}s timeout"
        log_info "Check pod status: kubectl get pods -n $NAMESPACE -l app=vllm-server"
        log_info "Check pod logs: kubectl logs -n $NAMESPACE -l app=vllm-server"
        return 1
    fi
}

# Status: Show current mode and GPU status
show_status() {
    echo -e "${BLUE}=== GPU Mode Status ===${NC}"
    echo ""

    # vLLM status
    local replicas
    replicas=$(get_vllm_replicas)
    local ready
    ready=$(get_vllm_ready)

    echo -e "${BLUE}vLLM Deployment:${NC}"
    echo "  Replicas: $replicas (Ready: ${ready:-0})"

    if [[ "$replicas" -eq 0 ]]; then
        echo -e "  Mode: ${YELLOW}Gaming Mode${NC} (vLLM scaled to 0)"
    elif [[ "${ready:-0}" -ge 1 ]]; then
        echo -e "  Mode: ${GREEN}ML Mode${NC} (vLLM running)"
    else
        echo -e "  Mode: ${YELLOW}Transitioning${NC} (vLLM starting)"
    fi

    # vLLM pods
    echo ""
    echo -e "${BLUE}vLLM Pods:${NC}"
    kubectl get pods -n "$NAMESPACE" -l app=vllm-server --no-headers 2>/dev/null | \
        awk '{printf "  %-40s %s\n", $1, $3}' || echo "  No pods found"

    # GPU status
    echo ""
    echo -e "${BLUE}GPU Status:${NC}"
    if command -v nvidia-smi &> /dev/null; then
        local gpu_mem
        gpu_mem=$(get_gpu_memory)
        echo "  Memory: ${gpu_mem} MiB (used/total)"

        # Show GPU processes
        echo ""
        echo -e "${BLUE}GPU Processes:${NC}"
        nvidia-smi --query-compute-apps=pid,name,used_memory --format=csv,noheader 2>/dev/null | \
            while read -r line; do
                if [[ -n "$line" ]]; then
                    echo "  $line"
                fi
            done || echo "  No GPU processes"
    else
        echo "  nvidia-smi not available"
    fi

    # Ollama fallback status
    echo ""
    echo -e "${BLUE}Ollama CPU Fallback:${NC}"
    local ollama_status
    ollama_status=$(check_ollama)
    if [[ "$ollama_status" == "available" ]]; then
        echo -e "  Status: ${GREEN}Available${NC}"
        # Show models
        local models
        models=$(kubectl exec -n "$NAMESPACE" deploy/ollama -- curl -s http://localhost:11434/api/tags 2>/dev/null | grep -o '"name":"[^"]*"' | cut -d'"' -f4 | tr '\n' ', ' | sed 's/,$//')
        if [[ -n "$models" ]]; then
            echo "  Models: $models"
        fi
    else
        echo -e "  Status: ${RED}Unavailable${NC}"
    fi
}

# Main
case "${1:-}" in
    gaming)
        gaming_mode
        ;;
    ml)
        ml_mode
        ;;
    status)
        show_status
        ;;
    *)
        echo "Usage: gpu-mode {gaming|ml|status}"
        echo ""
        echo "Commands:"
        echo "  gaming  - Activate Gaming Mode (scale vLLM to 0, release GPU)"
        echo "  ml      - Activate ML Mode (scale vLLM to 1, restore inference)"
        echo "  status  - Show current mode and GPU status"
        echo ""
        echo "Examples:"
        echo "  gpu-mode gaming   # Before playing Steam games"
        echo "  gpu-mode ml       # After gaming, restore ML inference"
        echo "  gpu-mode status   # Check current state"
        exit 1
        ;;
esac
