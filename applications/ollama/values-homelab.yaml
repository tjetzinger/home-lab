# Ollama Helm values for home-lab
# Story: 12.8 - Upgrade Ollama Model to Qwen 2.5 14B
# Epic: 12 - GPU/ML Inference Platform
#
# FRs:
# - FR8: Deploy applications using Helm charts
# - FR36: Deploy Ollama for LLM inference
# - FR104: Unified Qwen 2.5 14B for all inference
#
# NFRs:
# - NFR58: 95%+ valid JSON output
# - NFR61: Acceptable inference speed on CPU
# - NFR62: <60s classification latency on CPU
#
# Components: Ollama LLM inference service (Deployment)

# Ollama deployment configuration
replicaCount: 1  # Single instance (CPU-only MVP)

# Container image
image:
  repository: ollama/ollama
  pullPolicy: IfNotPresent
  tag: "latest"  # Use latest stable Ollama version

# Ollama-specific configuration
ollama:
  # Port Ollama is listening on
  port: 11434

  # GPU disabled for CPU-only MVP
  gpu:
    enabled: false

  # Models to pull at startup (empty for now, will pull manually in testing)
  models:
    pull: []
    run: []
    create: []
    clean: false

# Service configuration
service:
  type: ClusterIP  # Internal cluster access only
  port: 11434  # Ollama default API port
  annotations: {}
  labels:
    app.kubernetes.io/part-of: home-lab

# Resource limits (sized for qwen2.5:14b model inference)
resources:
  requests:
    cpu: 500m  # 0.5 CPU cores for inference
    memory: 4Gi  # 4GB RAM for model loading
  limits:
    cpu: 4000m  # 4 CPU cores max (bursting allowed)
    memory: 16Gi  # 16GB RAM for qwen2.5:14b model

# Persistent storage for models
persistentVolume:
  enabled: true  # Enable NFS-backed model storage
  storageClass: nfs-client  # Use NFS storage class (from Epic 2)
  size: 50Gi  # 50GB storage for models
  accessModes:
    - ReadWriteOnce  # RWO: Single node mount
  annotations: {}

# Additional environment variables
extraEnv:
  - name: OLLAMA_HOST
    value: "0.0.0.0:11434"  # Listen on all interfaces
  - name: OLLAMA_KEEP_ALIVE
    value: "-1"  # Keep model loaded indefinitely (avoids ~164s cold start)

# Labels for home-lab consistency
podLabels:
  app.kubernetes.io/part-of: home-lab
  app.kubernetes.io/component: llm-inference

# Pod annotations
podAnnotations: {}

# Security context
securityContext: {}
podSecurityContext: {}

# Node selector - deploy to k3s-worker-02 (20GB RAM for 14B model)
nodeSelector:
  kubernetes.io/hostname: k3s-worker-02

# Tolerations
tolerations: []

# Affinity rules
affinity: {}

# Deployment update strategy
updateStrategy:
  type: Recreate

# Probes
livenessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

readinessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 30
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 6
  successThreshold: 1

# Ingress disabled (will create IngressRoute separately)
ingress:
  enabled: false
