# Ollama Helm values for home-lab
# Story: 12.10 - Configure vLLM GPU Integration for Paperless-AI
# Story: 25.2 - Upgrade Ollama to Phi4-mini and Update LiteLLM
# Epic: 12 - GPU/ML Inference Platform
# Epic: 25 - Document Processing Pipeline Upgrade
#
# FRs:
# - FR8: Deploy applications using Helm charts
# - FR36: Deploy Ollama for LLM inference
# - FR206: phi4-mini (Microsoft 3.8B) deployed as CPU fallback model
# - FR207: LiteLLM updated with phi4-mini model path
#
# NFRs:
# - NFR58: 95%+ valid JSON output
# - NFR109: 70%+ classification accuracy on CPU fallback
# - NFR111: CPU classification within 60 seconds
#
# Note: Ollama serves as CPU fallback tier (Tier 2) in the LiteLLM inference chain.
# Primary model: phi4-mini (Microsoft Phi-4-mini 3.8B, ~2.5GB Q4)
# Also available: llama3.2:1b for lightweight/experimental use.
#
# Components: Ollama LLM inference service (Deployment)

# Ollama deployment configuration
replicaCount: 1  # Single instance (CPU-only MVP)

# Container image
image:
  repository: ollama/ollama
  pullPolicy: IfNotPresent
  tag: "latest"  # Use latest stable Ollama version

# Ollama-specific configuration
ollama:
  # Port Ollama is listening on
  port: 11434

  # GPU disabled for CPU-only MVP
  gpu:
    enabled: false

  # Models to pull at startup (empty for now, will pull manually in testing)
  models:
    pull: []
    run: []
    create: []
    clean: false

# Service configuration
service:
  type: ClusterIP  # Internal cluster access only
  port: 11434  # Ollama default API port
  annotations: {}
  labels:
    app.kubernetes.io/part-of: home-lab

# Resource limits (sized for phi4-mini ~2.5GB + llama3.2:1b)
resources:
  requests:
    cpu: 500m  # 0.5 CPU cores for inference
    memory: 2Gi  # 2GB RAM for slim model loading
  limits:
    cpu: 2000m  # 2 CPU cores max for slim models
    memory: 4Gi  # 4GB RAM for phi4-mini (~2.5GB loaded)

# Persistent storage for models
persistentVolume:
  enabled: true  # Enable NFS-backed model storage
  storageClass: nfs-client  # Use NFS storage class (from Epic 2)
  size: 50Gi  # 50GB storage for models
  accessModes:
    - ReadWriteOnce  # RWO: Single node mount
  annotations: {}

# Additional environment variables
extraEnv:
  - name: OLLAMA_HOST
    value: "0.0.0.0:11434"  # Listen on all interfaces
  - name: OLLAMA_KEEP_ALIVE
    value: "-1"  # Keep model loaded indefinitely (avoids ~164s cold start)

# Labels for home-lab consistency
podLabels:
  app.kubernetes.io/part-of: home-lab
  app.kubernetes.io/component: llm-inference

# Pod annotations
podAnnotations: {}

# Security context
securityContext: {}
podSecurityContext: {}

# Node selector - deploy to k3s-worker-02 (CPU fallback tier)
nodeSelector:
  kubernetes.io/hostname: k3s-worker-02

# Tolerations
tolerations: []

# Affinity rules
affinity: {}

# Deployment update strategy
updateStrategy:
  type: Recreate

# Probes
livenessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

readinessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 30
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 6
  successThreshold: 1

# Ingress disabled (will create IngressRoute separately)
ingress:
  enabled: false
