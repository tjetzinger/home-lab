# LiteLLM Configuration ConfigMap
# Story: 14.2 - Configure Three-Tier Fallback Chain
# Epic: 14 - LiteLLM Inference Proxy
#
# FRs:
# - FR113: LiteLLM proxy deployed in ml namespace providing unified OpenAI-compatible endpoint
# - FR114: LiteLLM configured with three-tier fallback (vLLM → Ollama → OpenAI)
# - FR116: Automatic failover to Ollama when vLLM health check fails
# - FR117: OpenAI API key stored as Kubernetes secret
#
# NFRs:
# - NFR65: Failover detection within 5 seconds
# - NFR66: LiteLLM proxy latency overhead <100ms
# - NFR67: Document processing continues (degraded) via fallback chain
# - NFR68: OpenAI only used when both vLLM and Ollama unavailable
#
# Three-tier fallback chain: vLLM (GPU) → Ollama (CPU) → OpenAI (cloud)

apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ml
  labels:
    app: litellm
    app.kubernetes.io/name: litellm
    app.kubernetes.io/instance: litellm
    app.kubernetes.io/part-of: home-lab
    app.kubernetes.io/managed-by: kubectl
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # Three-tier fallback: vLLM (GPU) → Ollama (CPU) → OpenAI (cloud)

    model_list:
      # Tier 1: vLLM on GPU worker (primary)
      # High performance, GPU-accelerated inference
      - model_name: vllm-qwen
        litellm_params:
          model: openai/Qwen/Qwen2.5-7B-Instruct-AWQ
          api_base: http://vllm-api.ml.svc.cluster.local:8000/v1
          api_key: sk-none  # vLLM doesn't require auth, but LiteLLM needs a value
          timeout: 3  # Short timeout for fast failover (NFR65: <5s detection)
        model_info:
          mode: chat

      # Tier 2: Ollama on CPU worker (secondary fallback)
      # Slower but runs on CPU, available during Gaming Mode
      - model_name: ollama-qwen
        litellm_params:
          model: ollama/qwen2.5:3b
          api_base: http://ollama.ml.svc.cluster.local:11434
          timeout: 120  # Extended for cold cache model loading (~60s) + inference
        model_info:
          mode: chat

      # Tier 3: OpenAI (tertiary fallback - cloud)
      # Last resort when all local inference is unavailable
      - model_name: openai-gpt4o
        litellm_params:
          model: gpt-4o-mini  # Cost-effective for document classification
          api_key: os.environ/OPENAI_API_KEY
          timeout: 30
        model_info:
          mode: chat

    # Router settings
    router_settings:
      routing_strategy: simple-shuffle
      num_retries: 1  # Reduced to speed up failover (NFR65: <5s)
      timeout: 60

    # General settings
    general_settings:
      # No master key for internal cluster use (Tailscale provides security)
      # master_key: os.environ/LITELLM_MASTER_KEY

    # LiteLLM settings - fallbacks define the failover chain
    litellm_settings:
      set_verbose: false
      drop_params: true  # Drop unsupported params instead of failing
      request_timeout: 30
      # Fallback chain: vLLM → Ollama → OpenAI (NFR68)
      # Client requests "vllm-qwen", auto-falls back to ollama-qwen, then openai-gpt4o
      fallbacks: [{"vllm-qwen": ["ollama-qwen"]}, {"ollama-qwen": ["openai-gpt4o"]}]
      allowed_fails: 1
      cooldown_time: 30
