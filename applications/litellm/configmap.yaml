# LiteLLM Configuration ConfigMap
# Story: 14.6 - Configure External Provider Parallel Models
# Story: 20.3 - Configure LiteLLM with DeepSeek-R1
# Story: 25.5 - Enable VLM OCR Pipeline via Remote Services
# Story: 26.1 - Configure LiteLLM with Ollama Pro Cloud Models
# Epic: 14 - LiteLLM Inference Proxy
# Epic: 20 - Reasoning Model Support (DeepSeek-R1)
# Epic: 25 - Document Processing Pipeline Upgrade
# Epic: 26 - Ollama Pro Cloud Model Integration
#
# FRs:
# - FR113: LiteLLM proxy deployed in ml namespace providing unified OpenAI-compatible endpoint
# - FR114: LiteLLM configured with three-tier fallback (vLLM → Ollama → OpenAI)
# - FR116: Automatic failover to Ollama when vLLM health check fails
# - FR117: OpenAI API key stored as Kubernetes secret
# - FR118: LiteLLM exposes Prometheus metrics for inference routing and fallback events
# - FR141: LiteLLM configured with DeepSeek-R1 as reasoning-tier model
# - FR142: Groq free tier as parallel model option (llama-3.3-70b-versatile, mixtral-8x7b-32768)
# - FR143: Google AI Studio (Gemini) free tier as parallel model option
# - FR144: Mistral API free tier as parallel model option
# - FR145: API keys for external providers stored via Kubernetes secrets
# - FR211: LiteLLM routes granite-docling requests to Ollama (Story 25.5)
# - FR215: OLLAMA_API_KEY stored as Kubernetes secret (Story 26.1)
# - FR216: Three Ollama Pro cloud models added to model_list (Story 26.1)
# - FR217: Cloud-primary fallback chains configured (Story 26.1)
# - FR218: openai-gpt4o removed from auto-fallback, explicit selection only (Story 26.1)
#
# NFRs:
# - NFR65: Failover detection within 5 seconds
# - NFR66: LiteLLM proxy latency overhead <100ms
# - NFR67: Document processing continues (degraded) via fallback chain
# - NFR68: OpenAI only used when both vLLM and Ollama unavailable
# - NFR69: LiteLLM health endpoint responds within 1 second for readiness probes
# - NFR83: External provider requests complete <5 seconds
# - NFR84: Rate limiting configured for free tier quotas
# - NFR121: Cloud model inference response within 60 seconds (Story 26.1)
# - NFR122: OLLAMA_API_KEY applied via kubectl patch only, never kubectl apply (Story 26.1)
# - NFR123: Cloud failover activates within 5 seconds (Story 26.1)
#
# Architecture:
# - Fallback chain: Cloud (Ollama Pro) → vLLM (GPU) → Ollama (CPU) - automatic failover
# - openai-gpt4o: explicit selection only, not in auto-fallback chain (FR218)
# - Parallel models: Groq, Gemini, Mistral - explicit selection, independent of fallback

apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ml
  labels:
    app: litellm
    app.kubernetes.io/name: litellm
    app.kubernetes.io/instance: litellm
    app.kubernetes.io/part-of: home-lab
    app.kubernetes.io/managed-by: kubectl
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # Three-tier fallback: vLLM (GPU) → Ollama (CPU) → OpenAI (cloud)

    model_list:
      # Tier 1: vLLM on GPU worker (primary)
      # High performance, GPU-accelerated inference
      # Updated: Story 25.1 - Qwen3-8B-AWQ replaces Qwen2.5-7B-Instruct-AWQ
      - model_name: vllm-qwen
        litellm_params:
          model: openai/Qwen/Qwen3-8B-AWQ
          api_base: http://vllm-api.ml.svc.cluster.local:8000/v1
          api_key: sk-none  # vLLM doesn't require auth, but LiteLLM needs a value
          timeout: 60  # Allow headroom for vLLM cold start; connection errors still fail fast
        model_info:
          mode: chat

      # Tier 1b: vLLM DeepSeek-R1 (reasoning mode) - FR141
      # Available only when gpu-mode is set to "r1"
      # Outputs reasoning in <think> tags before final answer
      - model_name: vllm-r1
        litellm_params:
          model: openai/casperhansen/deepseek-r1-distill-qwen-7b-awq
          api_base: http://vllm-api.ml.svc.cluster.local:8000/v1
          api_key: sk-none  # vLLM doesn't require auth, but LiteLLM needs a value
          timeout: 60  # Extended for longer reasoning chains
        model_info:
          mode: chat

      # Tier 2: Ollama on CPU worker (secondary fallback)
      # Slower but runs on CPU, available during Gaming Mode
      - model_name: ollama-qwen
        litellm_params:
          model: ollama/phi4-mini
          api_base: http://ollama.ml.svc.cluster.local:11434
          timeout: 300  # Extended for cold cache model loading (~60s) + large prompt inference on CPU
        model_info:
          mode: chat

      # Tier 3: OpenAI (tertiary fallback - cloud)
      # Last resort when all local inference is unavailable
      - model_name: openai-gpt4o
        litellm_params:
          model: gpt-4o-mini  # Cost-effective for document classification
          api_key: os.environ/OPENAI_API_KEY
          timeout: 30
        model_info:
          mode: chat

      # ============================================================
      # VLM/OCR MODEL (Story 25.5 - FR211)
      # NOT part of the text fallback chain - used by Docling VLM pipeline only
      # Usage: Docling sends VLM requests to "granite-docling" via LiteLLM
      # ============================================================

      - model_name: granite-docling
        litellm_params:
          model: ollama/ibm/granite-docling:258m
          api_base: http://ollama.ml.svc.cluster.local:11434
          timeout: 240  # VLM inference ~90s/page on CPU, allow headroom
        model_info:
          mode: chat

      # ============================================================
      # PARALLEL MODELS (Story 14.6 - FR142, FR143, FR144)
      # These are NOT part of the fallback chain - explicit selection only
      # Usage: Request "groq/llama-3.3-70b-versatile" directly
      # ============================================================

      # Groq Models (FR142) - Fast inference, 6000 req/day free tier
      # Rate limit: ~4 req/min sustained, monitor via Prometheus for daily quota
      - model_name: groq/llama-3.3-70b-versatile
        litellm_params:
          model: groq/llama-3.3-70b-versatile
          api_key: os.environ/GROQ_API_KEY
          timeout: 30
        model_info:
          mode: chat

      - model_name: groq/mixtral-8x7b-32768
        litellm_params:
          model: groq/mixtral-8x7b-32768
          api_key: os.environ/GROQ_API_KEY
          timeout: 30
        model_info:
          mode: chat

      # Google AI Studio / Gemini Models (FR143) - 1500 req/day free tier
      # Rate limit: ~1 req/min sustained, monitor via Prometheus for daily quota
      # Note: gemini-1.5 models deprecated April 2025, using gemini-2.x
      - model_name: gemini/gemini-2.0-flash
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY
          timeout: 30
        model_info:
          mode: chat

      - model_name: gemini/gemini-2.5-flash
        litellm_params:
          model: gemini/gemini-2.5-flash
          api_key: os.environ/GEMINI_API_KEY
          timeout: 60
        model_info:
          mode: chat

      # Mistral Models (FR144) - European provider, free tier available
      # Rate limit: varies by tier, monitor via Prometheus for quota compliance
      - model_name: mistral/mistral-small-latest
        litellm_params:
          model: mistral/mistral-small-latest
          api_key: os.environ/MISTRAL_API_KEY
          timeout: 30
        model_info:
          mode: chat

      # ============================================================
      # CLOUD MODELS (Story 26.1 - FR216)
      # Primary inference tier via Ollama Pro API
      # api_base: https://ollama.com — LiteLLM ollama_chat provider appends /api/chat internally
      # Falls back to local tier: vllm-qwen → ollama-qwen (FR217)
      # openai-gpt4o NOT in fallback chain — explicit selection only (FR218)
      # Model names verified via Ollama Pro /api/tags: no -cloud suffix required
      # ============================================================

      - model_name: cloud-kimi
        litellm_params:
          model: ollama_chat/kimi-k2.5
          api_base: https://ollama.com
          api_key: os.environ/OLLAMA_API_KEY
          timeout: 60
        model_info:
          mode: chat

      - model_name: cloud-minimax
        litellm_params:
          model: ollama_chat/minimax-m2.5
          api_base: https://ollama.com
          api_key: os.environ/OLLAMA_API_KEY
          timeout: 60
        model_info:
          mode: chat

      - model_name: cloud-qwen3-coder
        litellm_params:
          model: ollama_chat/qwen3-coder:480b
          api_base: https://ollama.com
          api_key: os.environ/OLLAMA_API_KEY
          timeout: 60
        model_info:
          mode: chat

    # Router settings
    router_settings:
      routing_strategy: simple-shuffle
      num_retries: 1  # Reduced to speed up failover (NFR65: <5s)
      timeout: 60

    # General settings
    general_settings:
      # No master key for internal cluster use (Tailscale provides security)
      # master_key: os.environ/LITELLM_MASTER_KEY

    # LiteLLM settings - fallbacks define the failover chain
    litellm_settings:
      set_verbose: false
      drop_params: true  # Drop unsupported params instead of failing
      request_timeout: 30
      # Prometheus metrics callback (FR118) - exposes /metrics endpoint
      callbacks: ["prometheus"]
      # Fallback chains: Cloud (Ollama Pro) → vLLM (GPU) → Ollama (CPU) (FR217, NFR123)
      # openai-gpt4o removed from auto-fallback — explicit selection only (FR218)
      fallbacks:
        - {"cloud-kimi":        ["cloud-minimax", "vllm-qwen", "ollama-qwen"]}
        - {"cloud-minimax":     ["vllm-qwen", "ollama-qwen"]}
        - {"cloud-qwen3-coder": ["vllm-qwen", "ollama-qwen"]}
        - {"vllm-qwen":         ["ollama-qwen"]}
      # openai-gpt4o: explicit parallel only — not in auto-fallback chain (FR218)
      # Context window fallback: phi4-mini supports 128K context vs Qwen3's 8K
      context_window_fallbacks: [{"vllm-qwen": ["ollama-qwen"]}]
      allowed_fails: 1
      cooldown_time: 30
