# LiteLLM Configuration ConfigMap
# Story: 14.6 - Configure External Provider Parallel Models
# Story: 20.3 - Configure LiteLLM with DeepSeek-R1
# Story: 25.5 - Enable VLM OCR Pipeline via Remote Services
# Epic: 14 - LiteLLM Inference Proxy
# Epic: 20 - Reasoning Model Support (DeepSeek-R1)
# Epic: 25 - Document Processing Pipeline Upgrade
#
# FRs:
# - FR113: LiteLLM proxy deployed in ml namespace providing unified OpenAI-compatible endpoint
# - FR114: LiteLLM configured with three-tier fallback (vLLM → Ollama → OpenAI)
# - FR116: Automatic failover to Ollama when vLLM health check fails
# - FR117: OpenAI API key stored as Kubernetes secret
# - FR118: LiteLLM exposes Prometheus metrics for inference routing and fallback events
# - FR141: LiteLLM configured with DeepSeek-R1 as reasoning-tier model
# - FR142: Groq free tier as parallel model option (llama-3.3-70b-versatile, mixtral-8x7b-32768)
# - FR143: Google AI Studio (Gemini) free tier as parallel model option
# - FR144: Mistral API free tier as parallel model option
# - FR145: API keys for external providers stored via Kubernetes secrets
# - FR211: LiteLLM routes granite-docling requests to Ollama (Story 25.5)
#
# NFRs:
# - NFR65: Failover detection within 5 seconds
# - NFR66: LiteLLM proxy latency overhead <100ms
# - NFR67: Document processing continues (degraded) via fallback chain
# - NFR68: OpenAI only used when both vLLM and Ollama unavailable
# - NFR69: LiteLLM health endpoint responds within 1 second for readiness probes
# - NFR83: External provider requests complete <5 seconds
# - NFR84: Rate limiting configured for free tier quotas
#
# Architecture:
# - Fallback chain: vLLM (GPU) → Ollama (CPU) → OpenAI (cloud) - automatic failover
# - Parallel models: Groq, Gemini, Mistral - explicit selection, independent of fallback

apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ml
  labels:
    app: litellm
    app.kubernetes.io/name: litellm
    app.kubernetes.io/instance: litellm
    app.kubernetes.io/part-of: home-lab
    app.kubernetes.io/managed-by: kubectl
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # Three-tier fallback: vLLM (GPU) → Ollama (CPU) → OpenAI (cloud)

    model_list:
      # Tier 1: vLLM on GPU worker (primary)
      # High performance, GPU-accelerated inference
      # Updated: Story 25.1 - Qwen3-8B-AWQ replaces Qwen2.5-7B-Instruct-AWQ
      - model_name: vllm-qwen
        litellm_params:
          model: openai/Qwen/Qwen3-8B-AWQ
          api_base: http://vllm-api.ml.svc.cluster.local:8000/v1
          api_key: sk-none  # vLLM doesn't require auth, but LiteLLM needs a value
          timeout: 60  # Allow headroom for vLLM cold start; connection errors still fail fast
        model_info:
          mode: chat

      # Tier 1b: vLLM DeepSeek-R1 (reasoning mode) - FR141
      # Available only when gpu-mode is set to "r1"
      # Outputs reasoning in <think> tags before final answer
      - model_name: vllm-r1
        litellm_params:
          model: openai/casperhansen/deepseek-r1-distill-qwen-7b-awq
          api_base: http://vllm-api.ml.svc.cluster.local:8000/v1
          api_key: sk-none  # vLLM doesn't require auth, but LiteLLM needs a value
          timeout: 60  # Extended for longer reasoning chains
        model_info:
          mode: chat

      # Tier 2: Ollama on CPU worker (secondary fallback)
      # Slower but runs on CPU, available during Gaming Mode
      - model_name: ollama-qwen
        litellm_params:
          model: ollama/phi4-mini
          api_base: http://ollama.ml.svc.cluster.local:11434
          timeout: 120  # Extended for cold cache model loading (~60s) + inference
        model_info:
          mode: chat

      # Tier 3: OpenAI (tertiary fallback - cloud)
      # Last resort when all local inference is unavailable
      - model_name: openai-gpt4o
        litellm_params:
          model: gpt-4o-mini  # Cost-effective for document classification
          api_key: os.environ/OPENAI_API_KEY
          timeout: 30
        model_info:
          mode: chat

      # ============================================================
      # VLM/OCR MODEL (Story 25.5 - FR211)
      # NOT part of the text fallback chain - used by Docling VLM pipeline only
      # Usage: Docling sends VLM requests to "granite-docling" via LiteLLM
      # ============================================================

      - model_name: granite-docling
        litellm_params:
          model: ollama/ibm/granite-docling:258m
          api_base: http://ollama.ml.svc.cluster.local:11434
          timeout: 240  # VLM inference ~90s/page on CPU, allow headroom
        model_info:
          mode: chat

      # ============================================================
      # PARALLEL MODELS (Story 14.6 - FR142, FR143, FR144)
      # These are NOT part of the fallback chain - explicit selection only
      # Usage: Request "groq/llama-3.3-70b-versatile" directly
      # ============================================================

      # Groq Models (FR142) - Fast inference, 6000 req/day free tier
      # Rate limit: ~4 req/min sustained, monitor via Prometheus for daily quota
      - model_name: groq/llama-3.3-70b-versatile
        litellm_params:
          model: groq/llama-3.3-70b-versatile
          api_key: os.environ/GROQ_API_KEY
          timeout: 30
        model_info:
          mode: chat

      - model_name: groq/mixtral-8x7b-32768
        litellm_params:
          model: groq/mixtral-8x7b-32768
          api_key: os.environ/GROQ_API_KEY
          timeout: 30
        model_info:
          mode: chat

      # Google AI Studio / Gemini Models (FR143) - 1500 req/day free tier
      # Rate limit: ~1 req/min sustained, monitor via Prometheus for daily quota
      # Note: gemini-1.5 models deprecated April 2025, using gemini-2.x
      - model_name: gemini/gemini-2.0-flash
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY
          timeout: 30
        model_info:
          mode: chat

      - model_name: gemini/gemini-2.5-flash
        litellm_params:
          model: gemini/gemini-2.5-flash
          api_key: os.environ/GEMINI_API_KEY
          timeout: 60
        model_info:
          mode: chat

      # Mistral Models (FR144) - European provider, free tier available
      # Rate limit: varies by tier, monitor via Prometheus for quota compliance
      - model_name: mistral/mistral-small-latest
        litellm_params:
          model: mistral/mistral-small-latest
          api_key: os.environ/MISTRAL_API_KEY
          timeout: 30
        model_info:
          mode: chat

    # Router settings
    router_settings:
      routing_strategy: simple-shuffle
      num_retries: 1  # Reduced to speed up failover (NFR65: <5s)
      timeout: 60

    # General settings
    general_settings:
      # No master key for internal cluster use (Tailscale provides security)
      # master_key: os.environ/LITELLM_MASTER_KEY

    # LiteLLM settings - fallbacks define the failover chain
    litellm_settings:
      set_verbose: false
      drop_params: true  # Drop unsupported params instead of failing
      request_timeout: 30
      # Prometheus metrics callback (FR118) - exposes /metrics endpoint
      callbacks: ["prometheus"]
      # Fallback chain: vLLM → Ollama → OpenAI (NFR68)
      # Client requests "vllm-qwen", auto-falls back to ollama-qwen, then openai-gpt4o
      fallbacks: [{"vllm-qwen": ["ollama-qwen"]}, {"ollama-qwen": ["openai-gpt4o"]}]
      allowed_fails: 1
      cooldown_time: 30
