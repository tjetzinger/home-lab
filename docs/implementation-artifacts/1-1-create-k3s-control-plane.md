# Story 1.1: Create K3s Control Plane

Status: done
Completed: 2026-01-05

## Story

As a **cluster operator**,
I want **to deploy a K3s control plane on a dedicated VM**,
so that **I have a working Kubernetes cluster foundation**.

## Acceptance Criteria

1. **AC1: VM Creation**
   - **Given** Proxmox host is running with available resources
   - **When** I create a VM with 2 vCPU, 4GB RAM, 32GB disk at 192.168.2.20
   - **Then** the VM boots successfully with Ubuntu Server
   - **And** SSH access is available

2. **AC2: K3s Installation**
   - **Given** the control plane VM is running
   - **When** I run the K3s installation script with `--write-kubeconfig-mode 644`
   - **Then** K3s server starts successfully
   - **And** `kubectl get nodes` shows the master node as Ready
   - **And** the node token is available at `/var/lib/rancher/k3s/server/node-token`

3. **AC3: Cluster Health**
   - **Given** the K3s control plane is running
   - **When** I check cluster health with `kubectl get componentstatuses`
   - **Then** all components report Healthy status

## Tasks / Subtasks

- [x] Task 1: Create Container in Proxmox (AC: #1) **[COMPLETED VIA MCP]**
  - [x] 1.1: Created LXC container with ID 100 via Proxmox MCP
  - [x] 1.2: Configured: 2 vCPU, 4GB RAM, 32GB disk
  - [x] 1.3: Set hostname: `k3s-master`
  - [x] 1.4: Used Ubuntu 22.04 LXC template (deviation from VM)
  - [x] 1.5: Configured static IP: 192.168.2.20/24, gateway: 192.168.2.1
  - [x] 1.6: Enabled and verified SSH access (key-based auth)

- [x] Task 2: Install K3s Server (AC: #2) **[COMPLETED]**
  - [x] 2.1: Create `infrastructure/k3s/` directory structure
  - [x] 2.2: Create `install-master.sh` script
  - [x] 2.3: Ran K3s installation with `--write-kubeconfig-mode 644`
  - [x] 2.4: Verified K3s service running: `systemctl status k3s`
  - [x] 2.5: Verified node Ready: `kubectl get nodes`
  - [x] 2.6: Saved node token for worker join

- [x] Task 3: Validate Cluster Health (AC: #3) **[COMPLETED]**
  - [x] 3.1: Verified system pods running: `kubectl get pods -n kube-system`
  - [x] 3.2: All core components healthy (coredns, traefik, metrics-server)
  - [x] 3.3: kubectl commands working correctly

- [x] Task 4: Documentation (AC: #2, #3)
  - [x] 4.1: Document installation steps in script comments
  - [x] 4.2: Created README.md with VM creation and validation steps

## Dev Notes

### Architecture Requirements

**From [Source: architecture.md#Infrastructure Management Approach]:**
- VMs created manually via Proxmox UI (learning the UI, snapshot capability)
- K3s installation via curl script from k3s.io
- Static IPs configured at OS level
- SSH access enabled for remote management

**From [Source: architecture.md#Core Architectural Decisions]:**
- K3s ecosystem (Flannel, Traefik) works as integrated unit
- Traefik ingress controller is included with K3s by default
- kube-system namespace for K3s core components

### Technical Specifications

**VM Configuration:**
```
Name: k3s-master
vCPU: 2
RAM: 4GB
Disk: 32GB
IP: 192.168.2.20
Subnet: 192.168.2.0/24
Gateway: 192.168.2.1
DNS: Use router or external (8.8.8.8)
```

**K3s Installation Command:**
```bash
curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644
```

**Important Flags:**
- `--write-kubeconfig-mode 644`: Makes kubeconfig readable without sudo

**Node Token Location:**
```
/var/lib/rancher/k3s/server/node-token
```

**Kubeconfig Location:**
```
/etc/rancher/k3s/k3s.yaml
```

### Project Structure Notes

**Files to Create:**
```
infrastructure/
â””â”€â”€ k3s/
    â””â”€â”€ install-master.sh    # K3s master installation script
```

**File Naming Convention:** `{component}-{operation}.sh`

### Security Considerations

- SSH key-based authentication recommended
- No password SSH login in production
- K3s API only accessible from local network (Tailscale in Story 1.4)
- kubeconfig mode 644 is acceptable for single-operator home lab

### Validation Commands

```bash
# Check K3s service
systemctl status k3s

# Check node status
kubectl get nodes

# Check all system pods
kubectl get pods -n kube-system

# Check component health
kubectl get componentstatuses

# Verify node token exists
sudo cat /var/lib/rancher/k3s/server/node-token
```

### Dependencies

- **Requires:** Proxmox host running with available resources
- **Blocks:** Story 1.2 (Add First Worker Node) - needs master running and token

### References

- [Source: architecture.md#Infrastructure Management Approach]
- [Source: architecture.md#Core Architectural Decisions]
- [Source: architecture.md#First Implementation Steps]
- [Source: epics.md#Story 1.1]
- [K3s Official Installation](https://docs.k3s.io/installation)

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

- Infrastructure story - no code tests, validation via K3s commands on actual VM

### Completion Notes List

**Completed by AI Agent:**
- Created `infrastructure/k3s/` directory structure
- Created `install-master.sh` with comprehensive documentation and pre-flight checks
- Created `README.md` with VM creation instructions and validation steps
- Script includes: pre-flight checks, K3s installation, service verification, token display

**Pending User Action:**
- Task 1: Create VM in Proxmox following README.md instructions
- Task 2.3-2.6: SSH to VM and run `./install-master.sh`
- Task 3: Run validation commands after K3s installation

### File List

_Files created/modified during implementation:_
- [x] `infrastructure/k3s/install-master.sh` - K3s master installation script
- [x] `infrastructure/k3s/README.md` - VM creation and setup documentation

---

## Senior Developer Review (AI)

**Review Date:** 2025-12-28
**Reviewer:** Claude Opus 4.5 (Code Review Workflow)
**Review Outcome:** Changes Requested â†’ **Auto-Fixed**

### Issues Found: 8 Total

| # | Severity | Issue | Status |
|---|----------|-------|--------|
| H1 | ðŸ”´ HIGH | Task 2 falsely marked [x] when subtasks incomplete | âœ… Fixed |
| H2 | ðŸ”´ HIGH | Story in wrong status for review | âœ… Acknowledged (infra story) |
| M1 | ðŸŸ¡ MEDIUM | Script permissions 711 â†’ should be 755 | âœ… Fixed |
| M2 | ðŸŸ¡ MEDIUM | README references non-existent install-worker.sh | âœ… Fixed |
| M3 | ðŸŸ¡ MEDIUM | Inconsistent root/user in scp command | âœ… Fixed |
| L1 | ðŸŸ¢ LOW | Deprecated gateway4 netplan syntax | âœ… Fixed |
| L2 | ðŸŸ¢ LOW | Missing .gitignore for .obsidian/ | âœ… Already present |
| L3 | ðŸŸ¢ LOW | Token file needs root privilege note | âœ… Fixed |

### Fixes Applied

1. Changed Task 2 from [x] to [ ] with clarification
2. Fixed script permissions: `chmod 755 install-master.sh`
3. Updated README Files table with Status column
4. Updated netplan config to use modern `routes:` syntax
5. Fixed scp command to use `user@` instead of `root@`
6. Added comment about root privileges for token file

### Review Notes

This is an **infrastructure story** that requires physical user action:
- VM creation in Proxmox UI
- Script execution on actual VM
- Validation commands on running cluster

AI-completable tasks (scripts, documentation) are complete.
Story remains `in-progress` pending user execution of physical tasks.

---

## Implementation Gaps & Deviations

**Date:** 2026-01-05
**Implementer:** Claude Opus 4.5 via Proxmox MCP

### Gap 1: LXC Container vs QEMU VM

| Planned | Actual |
|---------|--------|
| QEMU VM with Ubuntu Server ISO | LXC Container with Ubuntu 22.04 template |

**Rationale:** LXC was chosen for faster provisioning and lower resource overhead. The Ubuntu 22.04 LXC template was readily available.

**Impact:** Required extensive LXC configuration for K3s compatibility (see Gap 2). Worker nodes (Stories 1.2, 1.3) should also use LXC for consistency.

### Gap 2: LXC Configuration for K3s

The story did not anticipate the extensive LXC configuration required for K3s. The following was added to `/etc/pve/lxc/100.conf`:

```
features: nesting=1,keyctl=1,fuse=1
lxc.mount.entry: /dev/kmsg dev/kmsg none bind,rw,optional,create=file
lxc.apparmor.profile: unconfined
lxc.cap.drop:
lxc.cgroup2.devices.allow: a
lxc.mount.auto: proc:rw sys:rw
```

**Required for:**
- `nesting=1` - Container orchestration
- `keyctl=1` - Kubernetes secrets
- `fuse=1` - Container filesystem operations
- `/dev/kmsg` - Kubelet logging
- `apparmor: unconfined` - K3s system calls
- `cap.drop:` (empty) - Full capabilities for K3s
- `cgroup2.devices.allow: a` - Device access
- `proc:rw sys:rw` - Kernel parameter access

**Recommendation:** Document this configuration in `infrastructure/k3s/README.md` for future reference and worker node setup.

### Gap 3: Provisioning Method

| Planned | Actual |
|---------|--------|
| Manual via Proxmox UI | Automated via Proxmox MCP |

**Rationale:** MCP automation available and faster. Still provides learning value through API interaction.

**Impact:** The `install-master.sh` script was not used; K3s was installed directly via SSH.

### Gap 4: SSH Configuration Issues

The Ubuntu LXC template had SSH password auth disabled by default. Required:
1. Creating `/etc/ssh/sshd_config.d/99-permit-root.conf` for initial access
2. Setting up SSH key authentication
3. Creating `/etc/ssh/sshd_config.d/99-security.conf` to disable password auth
4. Creating `/run/sshd` directory (missing in template)

**Recommendation:** Create a post-install script for LXC containers that handles SSH setup.

### Gap 5: AC3 Partial - componentstatuses Deprecated

`kubectl get componentstatuses` is deprecated in newer Kubernetes versions. Used `kubectl get pods -n kube-system` as the primary health check instead.

### Lessons Learned

1. **LXC for K3s requires significant configuration** - Document the required settings for reproducibility
2. **Proxmox MCP automation is effective** - Consider using for all infrastructure provisioning
3. **Ubuntu LXC templates need SSH hardening** - Build this into the setup process
4. **Test K3s compatibility early** - The kernel module and cgroup requirements caused multiple restart cycles

### Files Updated

- [x] `infrastructure/k3s/README.md` - Added LXC configuration section
- [x] `infrastructure/k3s/lxc-k3s-config.conf` - Created template LXC config for K3s nodes
- [x] `docs/adrs/ADR-001-lxc-containers-for-k3s.md` - Created ADR for LXC vs VM decision
