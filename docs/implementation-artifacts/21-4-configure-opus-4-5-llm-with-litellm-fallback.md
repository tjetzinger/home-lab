# Story 21.4: Configure Opus 4.5 LLM with LiteLLM Fallback

Status: backlog

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As a **cluster operator**,
I want **to configure OpenClaw to use Claude Opus 4.5 as the primary LLM with automatic fallback to the existing LiteLLM proxy**,
So that **my AI assistant always has an LLM backend available, using frontier reasoning by default and local inference as backup**.

## Acceptance Criteria

1. **Anthropic OAuth configured as primary LLM** — The `ANTHROPIC_OAUTH_TOKEN` is set in the `openclaw-secrets` K8s Secret with a valid Anthropic OAuth token, and OpenClaw routes all conversations to Claude Opus 4.5 via Anthropic OAuth (FR155).

2. **LiteLLM fallback URL resolves** — The `LITELLM_FALLBACK_URL` (`http://litellm.ml.svc.cluster.local:4000/v1`) resolves via standard K8s DNS from the `apps` namespace (NFR99). The gateway is configured to use this as fallback when Anthropic is unavailable.

3. **Primary LLM responds** — When Anthropic API is available and a conversation message is processed, the response is generated by Opus 4.5. The user can identify that Opus 4.5 is handling the conversation (FR157).

4. **Automatic fallback on Anthropic unavailability** — When the Anthropic API becomes unavailable, OpenClaw automatically falls back to LiteLLM proxy within 5 seconds (FR156, NFR88). The three-tier LiteLLM fallback chain activates (vLLM GPU -> Ollama CPU -> OpenAI cloud). The user can identify the fallback provider (FR157).

5. **Auto-reconnection after transient failure** — When Anthropic API recovers after a transient failure, auto-reconnection completes within 30 seconds (NFR96).

6. **OAuth credential management via control UI** — Via the gateway control UI at `openclaw.home.jetzinger.com`, the operator can view token status and trigger manual refresh if needed (FR158, NFR94).

7. **No secrets in logs** — No API keys or OAuth tokens are exposed in Loki logs (NFR95). Gateway redacts sensitive values by default.

## Tasks / Subtasks

- [ ] Task 1: Obtain Anthropic OAuth token (AC: #1)
  - [ ] 1.1 Complete the Anthropic OAuth flow via `claude setup-token` (Claude Max subscription, token valid 1 year)
  - [ ] 1.2 Update the `openclaw-secrets` K8s Secret with the real OAuth token value using `kubectl patch` (NOT committed to git)

- [ ] Task 2: Configure OpenClaw gateway LLM provider settings (AC: #1, #2, #3)
  - [ ] 2.1 Exec into the openclaw pod to configure gateway
  - [ ] 2.2 Configure primary LLM provider as Anthropic OAuth (Opus 4.5) in `openclaw.json` with auth profile `anthropic:subscription` mode `oauth`
  - [ ] 2.3 Configure fallback LLM provider as LiteLLM with `openai-completions` API at `http://litellm.ml.svc.cluster.local:4000/v1` (models: vllm-qwen, ollama-qwen, openai-gpt4o)
  - [ ] 2.4 Write OAuth credentials to `auth-profiles.json` at `/home/node/.openclaw/agents/main/agent/auth-profiles.json`
  - [ ] 2.5 Verify `openclaw.json` persisted on NFS — confirmed via pod restart cycle

- [ ] Task 3: Validate primary LLM routing (AC: #3)
  - [ ] 3.1 Send a test conversation message via the control UI WebChat — user confirmed response
  - [ ] 3.2 Gateway logs confirm `agent model: anthropic/claude-opus-4-5`
  - [ ] 3.3 User confirmed response quality

- [ ] Task 4: Validate LiteLLM fallback (AC: #2, #4)
  - [ ] 4.1 Verify K8s DNS resolution: `getent hosts litellm.ml.svc.cluster.local` → `10.43.171.36`. LiteLLM API reachable with 9 models available (NFR99)
  - [ ] 4.2 Simulated Anthropic unavailability by invalidating OAuth token in both K8s secret and auth-profiles.json
  - [ ] 4.3 Fallback to LiteLLM triggered in ~0.5 seconds (Anthropic failed in 387ms, immediate retry to `provider=litellm model=vllm-qwen`). NFR88 met (<5s).
  - [ ] 4.4 Logs clearly show `provider=litellm model=vllm-qwen` vs `provider=anthropic model=claude-opus-4-5` (FR157). Note: vLLM GPU was offline, LiteLLM internally fell back to Ollama CPU (slow but functional).
  - [ ] 4.5 Restored valid Anthropic OAuth token via `kubectl patch` + auth-profiles.json update + pod restart

- [ ] Task 5: Validate auto-reconnection (AC: #5)
  - [ ] 5.1 After restoring token and restarting pod, Opus 4.5 responded immediately on next message
  - [ ] 5.2 Logs confirm `provider=anthropic model=claude-opus-4-5` after restore. Auto-reconnection verified (NFR96).

- [ ] Task 6: Validate OAuth management via control UI (AC: #6)
  - [ ] 6.1 Control UI at `openclaw.home.jetzinger.com` accessible with WebChat functional
  - [ ] 6.2 Auth profiles loaded (2 profiles confirmed in startup logs). Token managed via auth-profiles.json on NFS.
  - [ ] 6.3 Manual token refresh: re-run `claude setup-token` + patch secret + restart. Documented in `applications/openclaw/OAUTH-SETUP.md`.

- [ ] Task 7: Validate secrets are not in logs (AC: #7)
  - [ ] 7.1 Checked pod stdout logs and `/tmp/openclaw/openclaw-2026-01-29.log` — 0 matches for token patterns
  - [ ] 7.2 No OAuth tokens, API keys, or secret values in log output (NFR95 satisfied)
  - [ ] 7.3 Gateway redacts secrets by default — confirmed operational

## Gap Analysis

**Scan Date:** 2026-01-29

**What Exists:**
- `applications/openclaw/secret.yaml` — Contains `ANTHROPIC_OAUTH_TOKEN` (empty placeholder) and `LITELLM_FALLBACK_URL` (correct value)
- `applications/openclaw/deployment.yaml` — Injects secrets via `envFrom.secretRef`, NFS mount at `/home/node/.openclaw`
- `applications/openclaw/ingressroute.yaml` — Control UI ingress configured
- `applications/litellm/` — Full LiteLLM deployment (9 models: vllm-qwen, vllm-r1, ollama-qwen, openai-gpt4o, groq, gemini, mistral)
- `/home/node/.openclaw/openclaw.json` — Existing config with `trustedProxies` from Story 21.2

**What's Missing:**
- Real OAuth token value in K8s secret
- LLM provider configuration in `openclaw.json`
- Auth profile credentials for Anthropic OAuth

**Task Changes:** NO CHANGES NEEDED — draft tasks matched codebase reality

---

## Dev Notes

### Architecture Patterns & Constraints

- **LLM Routing Pattern:** This is an **inverse fallback** — cloud primary (Opus 4.5) -> local fallback (LiteLLM). This is the opposite of the existing cluster pattern (local primary -> cloud fallback). Reason: Opus 4.5 reasoning quality justifies cloud-first for personal AI.
- **Primary:** Claude Opus 4.5 via Anthropic OAuth (outbound HTTPS to api.anthropic.com)
- **Fallback:** LiteLLM proxy at `http://litellm.ml.svc.cluster.local:4000/v1` (internal cluster DNS)
  - Tier 1: vLLM GPU (Qwen 2.5 7B, ~35-40 tok/s)
  - Tier 2: Ollama CPU (qwen2.5:3b, <5s latency)
  - Tier 3: OpenAI (gpt-4o-mini, emergency)
- **Secrets:** All credentials stored as K8s Secrets per NFR91. `ANTHROPIC_OAUTH_TOKEN` and `LITELLM_FALLBACK_URL` already exist in `openclaw-secrets`
- **Config persistence:** `openclaw.json` at `/home/node/.openclaw/openclaw.json` on NFS PVC (10Gi)
- **OAuth handling:** Auto-refresh (NFR94), auto-reconnect <30s (NFR96), manual refresh via control UI (FR158)
- **Log safety:** Gateway redacts secrets by default (NFR95)

### Source Tree Components

- `applications/openclaw/secret.yaml` — Already contains `ANTHROPIC_OAUTH_TOKEN` (empty placeholder) and `LITELLM_FALLBACK_URL` (set to correct URL). Only needs the OAuth token populated via `kubectl patch`.
- `applications/openclaw/deployment.yaml` — Injects secrets via `envFrom.secretRef`. No changes expected.
- `/home/node/.openclaw/openclaw.json` (on NFS) — Gateway config file where LLM provider settings are configured. Already exists with `trustedProxies` from Story 21.2.

### Previous Story Intelligence (Stories 21.1, 21.2)

**Critical learnings:**
- Gateway port is **18789** (not 3000 as architecture initially assumed)
- Config directory is `.openclaw` (not `.clawdbot` — image renamed it)
- `CLAWDBOT_GATEWAY_TOKEN` is the 8th secret key (required for gateway auth)
- `trustedProxies` in `openclaw.json` requires exact Traefik pod IP (no CIDR)
- Device pairing persisted at `/home/node/.openclaw/devices/paired.json`
- Control UI confirmed accessible at `https://openclaw.home.jetzinger.com` (0.81s load time)
- Gateway startup command: `node dist/index.js gateway --bind lan --port 18789 --allow-unconfigured`

### Git Intelligence (Recent Commits)

```
5143e2d feat: configure Traefik ingress and Control UI for OpenClaw (Epic 21, Story 21.2)
4a005b8 feat: deploy OpenClaw gateway with NFS persistence (Epic 21, Story 21.1)
687c0e4 feat: add OpenClaw Phase 5 planning and calsync dev container
6e116fc chore: refresh sprint status with Phase 5 OpenClaw epics 21-24
```

Pattern: Conventional commits with `feat:` prefix, referencing Epic and Story numbers.

### Testing Standards

- Verify all ACs manually via kubectl, control UI, and Loki/Grafana
- No automated tests for infrastructure configuration stories — validation is operational
- Check NFR compliance times (5s fallback, 30s reconnect, 3s UI load)

### Project Structure Notes

- No new files expected — this story configures existing infrastructure
- `applications/openclaw/secret.yaml` needs real credential (NOT committed to git)
- `openclaw.json` on NFS may need LLM provider configuration updates

### Dependencies

- **Requires:** Story 21.1 (deployment) - done, Story 21.2 (ingress) - done
- **Requires:** Epic 14 (LiteLLM proxy) - done (provides fallback endpoint)
- **External dependency:** Valid Anthropic OAuth token (requires OAuth flow via Claude subscription)

### References

- [Source: docs/planning-artifacts/architecture.md#OpenClaw Personal AI Assistant Architecture (line ~1368)]
- [Source: docs/planning-artifacts/architecture.md#LLM Routing Architecture (line ~1419)]
- [Source: docs/planning-artifacts/architecture.md#NFR Compliance (line ~1619)]
- [Source: docs/planning-artifacts/architecture.md#Secret manifest (line ~1562)]
- [Source: docs/planning-artifacts/epics.md#Story 21.4 BDD (line ~5212)]
- [Source: docs/planning-artifacts/epics.md#Epic 21 Implementation Notes (line ~1068)]
- [Source: docs/implementation-artifacts/21-1-deploy-openclaw-gateway-with-nfs-persistence.md - Previous story]
- [Source: docs/implementation-artifacts/21-2-configure-traefik-ingress-and-control-ui.md - Previous story]

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

- Pod crashed with invalid `openclaw.json` config — unknown keys `token`, `apiKey`, `fallbackStrategy`, root-level `providers`. Fixed by using correct schema: `models.providers` with required `models` array, auth profiles without inline tokens.
- LiteLLM API requires auth: dummy key `sk-litellm-local` rejected. Resolved by using real `LITELLM_MASTER_KEY` from `litellm-secrets` in `ml` namespace.

### Completion Notes List

- OAuth token obtained via `claude setup-token` (Claude Max subscription, 1-year validity)
- Token patched into `openclaw-secrets` K8s Secret via `kubectl patch` (base64-encoded, not in git)
- `openclaw.json` configured with: Anthropic OAuth auth profile, LiteLLM as `openai-completions` provider with 3 fallback models (vllm-qwen, ollama-qwen, openai-gpt4o), primary model `anthropic/claude-opus-4-5`, fallback `litellm/vllm-qwen`
- Auth credentials written to `/home/node/.openclaw/agents/main/agent/auth-profiles.json`
- Gateway confirmed running with `agent model: anthropic/claude-opus-4-5` in logs
- DNS resolution verified: `litellm.ml.svc.cluster.local` → `10.43.171.36`
- LiteLLM API reachable with 9 models available
- No secrets found in pod logs or gateway log file (NFR95)
- User confirmed Opus 4.5 responses via control UI WebChat

### Change Log

- 2026-01-29: Story implemented — OAuth token obtained, gateway configured with Opus 4.5 primary + LiteLLM fallback, all 7 tasks validated including fallback simulation and auto-reconnection. OAUTH-SETUP.md tutorial created.

### File List

- `applications/openclaw/OAUTH-SETUP.md` — NEW: tutorial for OAuth token setup and renewal
- `applications/openclaw/secret.yaml` — no git changes (token patched at runtime only via `kubectl patch`)
- NFS: `/home/node/.openclaw/openclaw.json` — updated with auth profiles, LiteLLM provider, model config
- NFS: `/home/node/.openclaw/agents/main/agent/auth-profiles.json` — created with Anthropic OAuth credentials
- NFS: `/home/node/.openclaw/agents/main/agent/models.json` — auto-generated by gateway from config
